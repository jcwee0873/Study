{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c612c403",
   "metadata": {},
   "source": [
    "## NLP\n",
    "- 자연어 -> 컴퓨터 언어 (NLU)\n",
    "  - Text Classification\n",
    "  - POS Tagging\n",
    "  - Sentiment Analysis\n",
    "  - Machine Reading Comprehension\n",
    "  - Named Entity Recognition\n",
    "  - Semantic Parsing\n",
    "- 컴퓨터 언어 -> 자연어 (NLG)\n",
    "  - Language Modeling\n",
    "  - Article Generation\n",
    "- intersection\n",
    "  - Chatbot\n",
    "  - Summarization\n",
    "  - Question Answering\n",
    "  - Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0340543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ced08948",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- 정제 -> Tokenization -> Subword Segmentation(word2index) -> Batchify -> Prediction -> Detokenization(index2word)  \n",
    "\n",
    "### 데이터 수집\n",
    "- Purchase\n",
    "- Crwaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462bc47",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f3c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import re\n",
    "\n",
    "# def read_regex(fn):\n",
    "#     regexs = []\n",
    "\n",
    "#     f = open(fn, 'r')\n",
    "\n",
    "#     for line in f:\n",
    "#         if not line.startswith(\"#\"):\n",
    "#             tokens = line.split('\\t')\n",
    "\n",
    "#             if len(tokens) == 1:\n",
    "#                 tokens += [' ']\n",
    "\n",
    "#             tokens[0] = tokens[0][:-1] if tokens[0].endswith('\\n') else tokens[0]\n",
    "#             tokens[1] = tokens[1][:-1] if tokens[1].endswith('\\n') else tokens[1]\n",
    "\n",
    "#             regexs += [(tokens[0], tokens[1])]\n",
    "\n",
    "#     f.close()\n",
    "\n",
    "#     return regexs\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     fn = sys.argv[1]\n",
    "#     target_index = int(sys.argv[2])\n",
    "\n",
    "#     regexs = read_regex(fn)\n",
    "\n",
    "#     for line in sys.stdin:\n",
    "#         if line.strip() != \"\":\n",
    "#             columns = line.strip().split('\\t')\n",
    "\n",
    "#             for r in regexs:\n",
    "#                 columns[target_index] = re.sub(r'%s' % r[0], r[1], columns[target_index].strip())\n",
    "\n",
    "#             sys.stdout.write('\\t'.join(columns) + \"\\n\")\n",
    "#         else:\n",
    "#             sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8305598",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "- token 길이가 짧을경우\n",
    "  - Vocab 크기 감소, 희소성 문제 감소\n",
    "  - OoV(Out of Vocabulary, test 과정에서 train에서 못 본 단어, \\<UNK\\>로 치환)가 줄어듬\n",
    "  - Sequence의 길이가 김 -> 모델의 부담 증가\n",
    "- 길 수록\n",
    "  - 희소성 문제 증대\n",
    "  - OoV 상승 및 Sequence의 길이가 짧아 모델의 부담 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e10fe",
   "metadata": {},
   "source": [
    "- 정보량에 따라\n",
    "  - 빈도가 높을 경우 하나의 token으로 표현\n",
    "  - 빈도가 적을경우 잘게 쪼개어, 빈도가 높은 token으로 구성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63149c6",
   "metadata": {},
   "source": [
    "## Subword Segmentation\n",
    "- subword: 단어보다 작은 의미 단위\n",
    "- 이를 위해 별도의 subword 사전이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd571a9",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding(BPE) 알고리즘\n",
    "- training corpus를 활용하여 bpe 모델 학습 후, 코퍼스에 적용\n",
    "- pros\n",
    "  - 희소성을 통계에 기반하여 효과적으로 낮춤\n",
    "  - 언어별 특성에 대한 정보 없이 의미단위로 분절 가능\n",
    "  - OoV를 없앨 수 있다.\n",
    " - cons\n",
    "   - 학습데이터별로 BPE 모델도 생성됨  \n",
    "- Training\n",
    "  - 단어 사전 생성 (빈도 포함)\n",
    "  - Character 단위로 분절 후, pair 별 빈도 카운트\n",
    "  - 최빈도 pair를 골라 merge 수행\n",
    "  - pair별 빈도 카운트 업데이트\n",
    "  - 3 -> 반복  \n",
    "- 한국어의 경우 띄어쓰기가 제멋대로라, tokenizing 후 진행하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326738b",
   "metadata": {},
   "source": [
    "## Align Pararrel Corpus\n",
    "- 뉴스 기사, 영화 자막 등에서 대부분 문서 단위 matching 되어있고, 문장 단위는 X\n",
    "- ex. 영문 뉴스 기사 <-> 한국 뉴스 기사\n",
    "\n",
    "### Champollion\n",
    "- 단어 변역 사전에 기반하여, 사전을 최대한 만족하는 align을 찾음\n",
    "- 단어 번역 사전 -> Faebook MUSE\n",
    "  - 서로 다른 언어의 두 문서를 비교하여, Embedding을 비교하여 단어 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a2a50",
   "metadata": {},
   "source": [
    "## Mini-Batch\n",
    "- (batch_size, length, |V(one-hot vector)|) -> one-hot vector의 index값만 가져와도 됌\n",
    "- => (batch_size, length)\n",
    "- 생성과정에서 sequence length에 맞춰 정렬하는 이유\n",
    "  - mini-batch 내의 sqeunce 길이 차이가 많이 날 경우, \\<pad\\>만 채워진 문장 등으로 비효율\n",
    "  - mini-batch를 shuffling 하여 random하게 학습\n",
    "### 구성\n",
    "- corpus를 길이에 따라 정렬\n",
    "- token을 사전을 활용하여 str-index mapping\n",
    "- mini-batch 크기대로 chunk\n",
    "- 각 미니배치 별 텐서 구성 및 padding\n",
    "- 학습시 shuffling하여 iterative하게 반환\n",
    "\n",
    "### TorchText\n",
    "- text 로딩용 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98995b3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1df1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "\n",
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(\n",
    "        self, train_fn,\n",
    "        batch_size=64,\n",
    "        valid_ratio=.2,\n",
    "        device=-1,\n",
    "        max_vocab=999999,\n",
    "        min_freq=1,\n",
    "        use_eos=False,\n",
    "        shuffle=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.label = data.Field(\n",
    "            sequential=False,\n",
    "            use_vocab=True,\n",
    "            unk_token=None\n",
    "        )\n",
    "        \n",
    "        self.text = data.Field(\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=False,\n",
    "            eos_token='<EOS>' if use_eos else None\n",
    "        )\n",
    "\n",
    "        train, valid = data.TabularDataset(\n",
    "            path=train_fn,\n",
    "            format='tsv',\n",
    "            fields=[\n",
    "                ('label', self.label),\n",
    "                ('text', self.text)\n",
    "            ],\n",
    "        ).split(split_ratio=(1 - valid_ratio))\n",
    "\n",
    "        self.train_loader, self.valid_loader = data.BucketIterator.splits(\n",
    "            (train, valid),\n",
    "            batch_size=batch_size,\n",
    "            device='cuda:%d' % device if device >= 0 else 'cpu',\n",
    "            shuffle=shuffle,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch=True\n",
    "        )\n",
    "\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee044a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = DataLoader(\n",
    "    train_fn='./review.sorted.uniq.refined.tok..shuf.train.tsv',\n",
    "    batch_size=256,\n",
    "    valid_ratio=.2,\n",
    "    device=-1,\n",
    "    max_vocab=999999,\n",
    "    min_freq=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026af649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|train|=198023\n",
      "|valid|=49506\n"
     ]
    }
   ],
   "source": [
    "print(\"|train|=%d\" % len(loaders.train_loader.dataset))\n",
    "print(\"|valid|=%d\" % len(loaders.valid_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18374c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|vocab|=17078\n",
      "|label|=2\n"
     ]
    }
   ],
   "source": [
    "print(\"|vocab|=%d\" % len(loaders.text.vocab))\n",
    "print(\"|label|=%d\" % len(loaders.label.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a688bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 26])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(loaders.train_loader))\n",
    "\n",
    "print(data.text.shape)\n",
    "print(data.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7f14dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_default_unk_index',\n",
       " 'extend',\n",
       " 'freqs',\n",
       " 'itos',\n",
       " 'load_vectors',\n",
       " 'lookup_indices',\n",
       " 'set_vectors',\n",
       " 'stoi',\n",
       " 'unk_index',\n",
       " 'vectors']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(loaders.text.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e100e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stoi = string to index\n",
    "loaders.text.vocab.stoi['배송']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c467e8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'배송'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itos = index to string\n",
    "loaders.text.vocab.itos[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78198138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0: <unk>\t0\n",
      "    1: <pad>\t0\n",
      "    2: .\t239871\n",
      "    3: 고\t137806\n",
      "    4: 이\t132059\n",
      "    5: 하\t113380\n",
      "    6: 도\t93596\n",
      "    7: 네요\t90800\n",
      "    8: 좋\t87899\n",
      "    9: 에\t84665\n",
      "   10: 는\t79185\n",
      "   11: 가\t64409\n",
      "   12: 은\t60747\n",
      "   13: 는데\t52134\n",
      "   14: 아요\t49545\n",
      "   15: 게\t49397\n",
      "   16: 잘\t48316\n",
      "   17: 어요\t45911\n",
      "   18: 배송\t43585\n",
      "   19: 있\t41532\n",
      "   20: 했\t38792\n",
      "   21: 습니다\t38620\n",
      "   22: 안\t35249\n",
      "   23: 을\t34890\n",
      "   24: 한\t32937\n",
      "   25: ~\t30584\n",
      "   26: 구매\t28857\n",
      "   27: 같\t27811\n",
      "   28: 너무\t27328\n",
      "   29: 거\t26898\n",
      "   30: 합니다\t26865\n",
      "   31: 지\t26450\n",
      "   32: ..\t24535\n",
      "   33: 어\t23914\n",
      "   34: !\t23645\n",
      "   35: ,\t23612\n",
      "   36: 다\t23467\n",
      "   37: 가격\t22765\n",
      "   38: 되\t22271\n",
      "   39: ?\t21930\n",
      "   40: 것\t21315\n",
      "   41: 들\t20855\n",
      "   42: 제품\t20774\n",
      "   43: 았\t20634\n",
      "   44: 으로\t20523\n",
      "   45: 쓰\t20198\n",
      "   46: 아\t20051\n",
      "   47: 만\t19993\n",
      "   48: 로\t19662\n",
      "   49: 받\t19629\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    word = loaders.text.vocab.itos[i]\n",
    "    print('%5d: %s\\t%d' % (i, word, loaders.text.vocab.freqs[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c5485e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1224,   20,   13,  390,  325, 6806,   54,   90,  419,   10,    0,  588,\n",
      "         143,  991,  314,  191, 1473,  150,  148,   40,   27,  105,  808,  718,\n",
      "         324,    1])\n"
     ]
    }
   ],
   "source": [
    "print(data.text[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb99b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혹시나 했 는데 별루 임 싸우 나 에서 나오 는 <unk> 로션 정도 50 대 ㅋㅋㅋ 아저씨 용 인 것 같 음 향도 영 ~~~ <pad>\n"
     ]
    }
   ],
   "source": [
    "x = data.text[-1]\n",
    "line = []\n",
    "for x_i in x:\n",
    "    line += [loaders.text.vocab.itos[x_i]]\n",
    "    \n",
    "print(' '.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24883d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
